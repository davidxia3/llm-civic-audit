{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d4f6f8",
   "metadata": {},
   "source": [
    "# Module 1: Agenda Processing\n",
    "- Step 1: Downloading agendas .pdf files\n",
    "- Step 2: Extracting text from agenda .pdf files\n",
    "- Step 3: Segment agenda text into topic segments\n",
    "- Step 4: Fetch links to legislation\n",
    "- Step 5: Follow legislation links to obtain legislative text\n",
    "- Step 6: Match legislative text to agenda topic\n",
    "- Step 7: MANUAL matching of expert-written headlines to agenda topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d2926",
   "metadata": {},
   "source": [
    "Specify the week of meetings to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93900630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EDIT THIS\n",
    "MONDAY_DATE = \"YYYYMMDD\"\n",
    "FRIDAY_DATE = \"YYYYMMDD\"\n",
    "#### EDIT THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "WEEK = (MONDAY_DATE, FRIDAY_DATE)\n",
    "START_DATE = datetime.strptime(WEEK[0], \"%Y%m%d\")\n",
    "END_DATE = datetime.strptime(WEEK[1], \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28a1ed",
   "metadata": {},
   "source": [
    "## Step 1: Downloading agenda files from official website URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e3d41",
   "metadata": {},
   "source": [
    "The folder `___input/agenda_url/` should contain a `.txt` file for each city council meeting. The `.txt` file should contain only the Legistar URL of the specific meeting's agenda. Each `.txt` file should be named such that the first 8 characters are the meeting date in YYYYMMDD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_AGENDA_PATH = Path(\"../___input/agenda_url/\")\n",
    "assert INPUT_AGENDA_PATH.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02975a0d",
   "metadata": {},
   "source": [
    "The agenda `.pdf` files will be saved to `_interim/agenda_raw/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_AGENDA_PATH = Path(\"../_interim/agenda_raw/\")\n",
    "RAW_AGENDA_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56424a25",
   "metadata": {},
   "source": [
    "Download agendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03050ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_file in INPUT_AGENDA_PATH.glob(\"*.txt\"):\n",
    "\n",
    "    # check if meeting took place in specified week\n",
    "    date_str = txt_file.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "\n",
    "\n",
    "    # open .txt file\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        page_url = f.read().strip()\n",
    "\n",
    "        \n",
    "    # extract URL from file\n",
    "    if not page_url:\n",
    "        print(f\"!!!! ISSUE WITH URL IN FILE: {txt_file}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # load url\n",
    "    try:\n",
    "        response = requests.get(page_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"!!!! WEBSITE FAIL: {page_url}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # find .pdf file\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    HTML_AGENDA_ID = \"ctl00_ContentPlaceHolder1_hypMinutes\"\n",
    "    link_tag = soup.find(\"a\", id=HTML_AGENDA_ID)\n",
    "    if not link_tag or not link_tag.get(\"href\"):\n",
    "        print(f\"!!! PDF NOT FOUND: {page_url}\")\n",
    "        continue\n",
    "    pdf_url = urljoin(page_url, link_tag[\"href\"])\n",
    "\n",
    "\n",
    "    # download .pdf file\n",
    "    try:\n",
    "        pdf_response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        pdf_response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"!!! PDF DOWNLOAD FAIL: {pdf_url}\")\n",
    "        continue\n",
    "\n",
    "    # save to destination\n",
    "    output_filename = RAW_AGENDA_PATH / (txt_file.stem + \".pdf\")\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        f.write(pdf_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff195a5c",
   "metadata": {},
   "source": [
    "## Step 2: Extract text from agenda .pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe7c00",
   "metadata": {},
   "source": [
    "The folder `_interim/agenda_raw/` should now contain a .pdf for each meeting agenda. The next step is to extract the text from these .pdf files and save to a .txt file in the folder `_interim/agenda_processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert RAW_AGENDA_PATH.exists()\n",
    "PROCESSED_AGENDA_PATH = Path(\"../_interim/agenda_processed/\")\n",
    "PROCESSED_AGENDA_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15123876",
   "metadata": {},
   "source": [
    "Utilize PDFplumber library to extract text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_file in RAW_AGENDA_PATH.glob(\"*.pdf\"):\n",
    "\n",
    "    # check if meeting took place in specified week\n",
    "    date_str = txt_file.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "\n",
    "\n",
    "    # use PDFplumber to extract text\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        all_text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                all_text += page_text + \"\\n\\n\"\n",
    "\n",
    "    output_path = PROCESSED_AGENDA_PATH / f\"{pdf_file.stem}.txt\"\n",
    "    output_path.write_text(all_text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977872b",
   "metadata": {},
   "source": [
    "## Step 3: Segment agenda texts into topic segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c41f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e84e6",
   "metadata": {},
   "source": [
    "The agenda is segmented using Claude into topic segments and are saved as a `.csv` file in the `_interim/agenda_segments/` folder. First, load the Claude API key and client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUDE_API_KEY = os.getenv(\"CLAUDE_KEY\")\n",
    "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
    "\n",
    "assert PROCESSED_AGENDA_PATH.exists()\n",
    "AGENDA_SEGMENTS_PATH = Path(\"../_interim/agenda_segments/\")\n",
    "AGENDA_SEGMENTS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ad72a",
   "metadata": {},
   "source": [
    "Function to build Claude prompt from agenda text. Claude should return a JSON parsable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cee778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agenda_segmentation_prompt(agenda_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a professional city council meeting assistant.\n",
    "\n",
    "Given the full raw text of a meeting agenda, segment it into distinct agenda items. For each item:\n",
    "\n",
    "1. Include the full agenda item title (e.g., \"Bill 2023-114: Amending the zoning regulations\").\n",
    "2. Each **bill, paper, resolution, or ordinance** (e.g., \"ORD. 2023-114\", \"RES. 2023-R016\", \"PAPER #412\") counts as a **separate agenda item**, even if multiple items fall under the same section.\n",
    "3. If a bill number or ordinance number appears, include it as part of the agenda item title.\n",
    "4. Under each agenda item, include **all the text** that falls under it until the next agenda item begins.\n",
    "5. Keep the original wording and formatting. Do not summarize or shorten the text.\n",
    "6. Do not skip or omit any part of the agenda. This includes routine items such as “Roll Call,” “Public Comment,\" and other procedural sections.\n",
    "\n",
    "Return the segmented agenda as a JSON array of strings. Each string is one agenda item with its full text.\n",
    "\n",
    "**Important:** Return **only** the JSON array of strings with no additional text, explanation, or commentary. The output must be a valid JSON array.\n",
    "\n",
    "[\n",
    "  \"[Agenda item title]\\\\n   [Full text under the item]\",\n",
    "  \"[Next agenda item]\\\\n   [Full text under the item]\",\n",
    "  ...\n",
    "]\n",
    "\n",
    "Agenda:\n",
    "\\\"\\\"\\\"{agenda_text}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774f3d1",
   "metadata": {},
   "source": [
    "Prompt Claude and parse the string as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86816f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in PROCESSED_AGENDA_PATH.glob(\"*.txt\"):\n",
    "\n",
    "    # check if meeting took place in specified week\n",
    "    date_str = file_path.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "\n",
    "    # read processed agenda .txt file\n",
    "    file_stem = file_path.stem\n",
    "    output_path = AGENDA_SEGMENTS_PATH / f\"{file_stem}.csv\"\n",
    "    text = file_path.read_text(encoding='utf-8')\n",
    "\n",
    "\n",
    "    # build prompt and obtain JSON string\n",
    "    prompt = build_agenda_segmentation_prompt(text)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-latest\",\n",
    "        max_tokens=8192,\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    json_string = response.content[0].text.strip()\n",
    "\n",
    "\n",
    "    # try to parse the JSON string and save it if parsable\n",
    "    try:\n",
    "        segments = json.loads(json_string)\n",
    "        with output_path.open(\"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"agenda_segment\"])\n",
    "            for segment in segments:\n",
    "                writer.writerow([segment])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"!!!! LLM OUTPUT NOT JSON PARSABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf0707",
   "metadata": {},
   "source": [
    "## Step 4: Fetching legislation links for each agenda segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0df912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db45c7",
   "metadata": {},
   "source": [
    "Finds all links to official legislation text for each meeting and stores as a `.csv` file in the `../_interim/legislation/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert INPUT_AGENDA_PATH.exists()\n",
    "LEGISLATION_PATH = Path(\"../_interim/legislation/\")\n",
    "LEGISLATION_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_LEGISLATION_TABLE_ID = \"ctl00_ContentPlaceHolder1_gridMain_ctl00\"\n",
    "for txt_file in INPUT_AGENDA_PATH.glob(\"*.txt\"):\n",
    "\n",
    "    # check if meeting took place in specified week\n",
    "    date_str = txt_file.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # extract url from .txt file\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        page_url = f.read().strip()\n",
    "    if not page_url:\n",
    "        print(f\"!!!! ISSUE WITH URL IN FILE: {txt_file}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # load webpage\n",
    "    try:\n",
    "        response = requests.get(page_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"!!! WEBPAGE FAIL: {page_url}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # extract table from webpage using the html id\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", id=HTML_LEGISLATION_TABLE_ID)\n",
    "    if not table:\n",
    "        print(f\"!!!! NO TABLE WITH ID '{HTML_LEGISLATION_TABLE_ID}' FOUND.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    # extract and save URLs of legislation and their corresponding item\n",
    "    output_csv_path = LEGISLATION_PATH / f\"{txt_file.stem}.csv\"\n",
    "    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"item\", \"link\"])\n",
    "\n",
    "\n",
    "        # iterate table rows\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if not cells:\n",
    "                continue\n",
    "\n",
    "            first_col = cells[0]\n",
    "            text = re.sub(r\"[a-zA-Z\\s]\", \"\", first_col.get_text(strip=True))\n",
    "            links = first_col.find_all(\"a\", href=True)\n",
    "\n",
    "            for a in links:\n",
    "                href = urljoin(page_url, a[\"href\"])\n",
    "                if text and href:\n",
    "                    writer.writerow([text, href])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7dee8",
   "metadata": {},
   "source": [
    "## Step 5: Follow links to legislation and record the legislative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a025255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53d7d",
   "metadata": {},
   "source": [
    "Follow the links to each legislation and record the legislative text in the agenda segments file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert LEGISLATION_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "firefox_options = Options()\n",
    "firefox_options.add_argument(\"--width=1920\")\n",
    "firefox_options.add_argument(\"--height=1080\")\n",
    "\n",
    "driver = webdriver.Firefox(options=firefox_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_LEGISLATION_TAB_XPATH = '//li[contains(@class, \"rtsLI\") and contains(@class, \"rtsLast\")]//span[contains(@class, \"rtsTxt\") and text()=\"Text\"]/ancestor::li'\n",
    "HTML_LEGISLATION_TEXT_ID = \"ctl00_ContentPlaceHolder1_pageText\"\n",
    "\n",
    "try:\n",
    "    csv_files = list(LEGISLATION_PATH.glob(\"*.csv\"))\n",
    "    for csv_file in csv_files:\n",
    "\n",
    "        # check if meeting took place in specified week\n",
    "        date_str = csv_file.stem[:8]\n",
    "        try:\n",
    "            file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        except ValueError:\n",
    "            # skip, meeting does not valid date\n",
    "            continue\n",
    "        if file_date < START_DATE or file_date > END_DATE:\n",
    "            # skip, meeting did not take place in specified week\n",
    "            continue\n",
    "\n",
    "\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if \"link\" not in df.columns:\n",
    "            print(f\"!!! DID NOT DO STEP 4: {csv_file}.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        texts = []\n",
    "        for idx, url in enumerate(df[\"link\"]):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                wait = WebDriverWait(driver, 5)\n",
    "\n",
    "                # click the \"Text\" tab\n",
    "                tab_element = wait.until(EC.element_to_be_clickable((By.XPATH, HTML_LEGISLATION_TAB_XPATH)))\n",
    "                tab_element.click()\n",
    "\n",
    "\n",
    "                # check and access element containing text\n",
    "                content_div = wait.until(EC.visibility_of_element_located((By.ID, HTML_LEGISLATION_TEXT_ID)))\n",
    "                text_content = content_div.text.strip()\n",
    "\n",
    "                # check for \"Click here for full text\" link\n",
    "                try:\n",
    "                    full_text_link = content_div.find_element(By.LINK_TEXT, \"Click here for full text\")\n",
    "                    if full_text_link:\n",
    "                        full_text_url = full_text_link.get_attribute(\"href\")\n",
    "                        print(f\"    Found full text link, navigating to {full_text_url}\")\n",
    "                        driver.get(full_text_url)\n",
    "\n",
    "            \n",
    "                        content_div = wait.until(EC.visibility_of_element_located((By.ID, HTML_LEGISLATION_TEXT_ID)))\n",
    "                        text_content = content_div.text.strip()\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    # text not long enough to have \"Click here for full text\" link\n",
    "                    pass\n",
    "\n",
    "                print(f\"retrieved text length: {len(text_content)}\")\n",
    "                texts.append(text_content)\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"!!! ERROR PROCESSING URL: {url}\")\n",
    "\n",
    "                # default value\n",
    "                texts.append(\"NO_LEGISLATION\")\n",
    "\n",
    "            time.sleep(1) \n",
    "\n",
    "\n",
    "        df[\"text\"] = texts\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fd406",
   "metadata": {},
   "source": [
    "## Step 6: Match legislation texts to agenda segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62255fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert AGENDA_SEGMENTS_PATH.exists()\n",
    "assert LEGISLATION_PATH.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d79cd2",
   "metadata": {},
   "source": [
    "Using the agenda item, match the legislation text to the correct agenda topic and add the legislatition text to the agenda segments file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for leg_file in LEGISLATION_PATH.glob(\"*.csv\"):\n",
    "\n",
    "    # check if meeting took place in specified week\n",
    "    date_str = leg_file.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    aseg_file = AGENDA_SEGMENTS_PATH / leg_file.name\n",
    "    \n",
    "    # read .csv files\n",
    "    aseg_df = pd.read_csv(aseg_file)\n",
    "    leg_df = pd.read_csv(leg_file)\n",
    "\n",
    "    # default value\n",
    "    aseg_df[\"matched_legislation\"] = \"NO_LEGISLATION\"\n",
    "\n",
    "    # find matches\n",
    "    for _, leg_row in leg_df.iterrows():\n",
    "        for aseg_idx, aseg_row in aseg_df.iterrows():\n",
    "            if leg_row[\"item\"] in aseg_row[\"agenda_segment\"]:\n",
    "                # set to legislation text if default value and append if already been changed\n",
    "                if aseg_df.at[aseg_idx, \"matched_legislation\"] == \"NO_LEGISLATION\":\n",
    "                    aseg_df.at[aseg_idx,\"matched_legislation\"] = leg_row[\"text\"]\n",
    "                else:\n",
    "                    aseg_df.at[aseg_idx,\"matched_legislation\"] = aseg_df.at[aseg_idx,\"matched_legislation\"] + leg_row[\"text\"]\n",
    "        \n",
    "\n",
    "    aseg_df.to_csv(aseg_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcf082",
   "metadata": {},
   "source": [
    "## Step 7: MANUAL matching of expert written headlines to agenda topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04037e",
   "metadata": {},
   "source": [
    "Manually align the expert written headlines to the agenda topic that is covered by the headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../_interim/agenda_segments/20250401_REG.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[23, \"true_headline\"] = \"\"\"Open-Ended Professional Services Contracts Add 27 Unspecified New Vendors\"\"\"\n",
    "df.loc[27, \"true_headline\"] = \"\"\"Portable Restroom Back Payment Sparks Inquiries into the Prevalence of Closed Public Restrooms in City Parks\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250401_REG.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250402_STA.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[12, \"true_headline\"] = \"\"\"Council to Approve Funding for Recruitment Video Aimed at Addressing Police Shortages\"\"\"\n",
    "df.loc[17, \"true_headline\"] = \"\"\"14 Acre Bakery Square Expansion Moves Forward with Two Key Amendments Ready for Public Hearing\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250402_STA.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250408_POS.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[0, \"true_headline\"] = \"\"\"Mayor's Vision Zero Plan Appears on Track, Still Awaiting Traffic Fatality Data\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250408_POS.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250408_REG.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[8, \"true_headline\"] = \"\"\"Council Seeks State Assistance in Eliminating the City’s Litter and Illegal Dumpsites\"\"\"\n",
    "df.loc[10, \"true_headline\"] = \"\"\"Untested Sexual Assault Kits to Be Processed\"\"\"\n",
    "df.loc[32, \"true_headline\"] = \"\"\"Council to Approve Funding for Recruitment Video Aimed at Addressing Police Shortages\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250408_REG.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250409_STA.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[13, \"true_headline\"] = \"\"\"City Receives State Funds for New Lighting Along Penn Ave in East Liberty\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250409_STA.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250415_REG.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[8, \"true_headline\"] = \"\"\"$40,000 Contract with Law Firm for “Immigration Matters” Authorized\"\"\"\n",
    "df.loc[22, \"true_headline\"] = \"\"\"City Receives State Funds for New Lighting Along Penn Ave in East Liberty\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250415_REG.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250416_STA.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[2, \"true_headline\"] = \"\"\"Council Reopens Discussion of the Process to Purchase City Property After a Rescinded Sale\"\"\"\n",
    "df.loc[9, \"true_headline\"] = \"\"\"Untested Sexual Assault Kits to Be Processed\"\"\"\n",
    "df.loc[14, \"true_headline\"] = \"\"\"Proposed July 2024 Contract for Shelter Services Held Another Five Weeks\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250416_STA.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250422_POS.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250422_POS.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250422_REG.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[18, \"true_headline\"] = \"\"\"Riverlife Barge Venue to Dock at Allegheny Landing Park\"\"\"\n",
    "df.loc[19, \"true_headline\"] = \"\"\"Bill to Allow Pollinator Gardens and Natural Landscapes Introduced\"\"\"\n",
    "df.loc[21, \"true_headline\"] = \"\"\"Challenged Historic Designation for Former Gay Bar to Receive Public Hearing\"\"\"\n",
    "df.loc[38, \"true_headline\"] = \"\"\"Untested Sexual Assault Kits to Be Processed\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250422_REG.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250423_PUB.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[1, \"true_headline\"] = \"\"\"'Tiny Lots' Approved After Spirited Debate\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250423_PUB.csv\", index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../_interim/agenda_segments/20250423_STA.csv\")\n",
    "df[\"true_headline\"] = \"NO_HEADLINE\"\n",
    "df.loc[5, \"true_headline\"] = \"\"\"$40,000 Contract with Law Firm for “Immigration Matters” Authorized\"\"\"\n",
    "df.to_csv(\"../_interim/agenda_segments/20250423_STA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390afdc4",
   "metadata": {},
   "source": [
    "This concludes Module 1: Agenda Processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
