{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744e40fc",
   "metadata": {},
   "source": [
    "# Module 5: Auditing and Evaluating LLM Performance\n",
    "- Metric 1: Headline rank difference between LLM-generated and expert-written headlines\n",
    "- Metric 2: Top 3/5 recall rate from LLM and expert selections\n",
    "- Metric 3: Average rank of LLM and expert selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11b451",
   "metadata": {},
   "source": [
    "Define the date range to evaluate LLM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79924470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EDIT THIS\n",
    "WEEKS = [\n",
    "    (\"YYYYMMDD\", \"YYYYMMDD\"),\n",
    "    (\"YYYYMMDD\", \"YYYYMMDD\"),\n",
    "    (\"YYYYMMDD\", \"YYYYMMDD\"),\n",
    "    (\"YYYYMMDD\", \"YYYYMMDD\")\n",
    "]\n",
    "#### EDIT THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63fff1",
   "metadata": {},
   "source": [
    "## Metric 1: Headline rank difference\n",
    "Computes and displays the average rank difference between LLM generated headlines and expert written headlines, for the same controlled topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321909c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97728dc8",
   "metadata": {},
   "source": [
    "The folder `_interim/headlines/` should contain a `.csv` file of all headlines for each week. The rankings used are the LLM-generated and expert-written headlines ranked together by human evaluators. These are in the three folders:\n",
    "- `_interim/rankings/claude_manual/`\n",
    "- `_interim/rankings/gemini_manual/`\n",
    "- `_interim/rankings/openai_manual/`\n",
    "\n",
    "The figures will be saved to the folder `results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINES_PATH = Path(\"../_interim/headlines/\")\n",
    "assert HEADLINES_PATH.exists()\n",
    "CLAUDE_MANUAL_RANKING_PATH = Path(\"../_interim/rankings/claude_manual/\")\n",
    "assert CLAUDE_MANUAL_RANKING_PATH.exists()\n",
    "GEMINI_MANUAL_RANKING_PATH = Path(\"../_interim/rankings/gemini_manual/\")\n",
    "assert GEMINI_MANUAL_RANKING_PATH.exists()\n",
    "OPENAI_MANUAL_RANKING_PATH = Path(\"../_interim/rankings/openai_manual/\")\n",
    "assert OPENAI_MANUAL_RANKING_PATH.exists()\n",
    "RESULTS_PATH = Path(\"../results/\")\n",
    "assert RESULTS_PATH.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df61841",
   "metadata": {},
   "source": [
    "Compute and display average rank displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67902850",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"axes.unicode_minus\": False\n",
    "})\n",
    "\n",
    "llms = [\"claude\", \"gemini\", \"openai\"]\n",
    "llm_to_abbrev = {\"claude\": \"C\", \"gemini\": \"G\", \"openai\": \"O\"}\n",
    "llm_to_manual_folder = {\"claude\": CLAUDE_MANUAL_RANKING_PATH,\n",
    "                        \"gemini\": GEMINI_MANUAL_RANKING_PATH,\n",
    "                        \"openai\": OPENAI_MANUAL_RANKING_PATH}\n",
    "\n",
    "# displacement tracking\n",
    "displacements = {llm: 0 for llm in llms}\n",
    "counts = {llm: 0 for llm in llms}\n",
    "\n",
    "# process each week\n",
    "for week in WEEKS:\n",
    "    week_str = f\"{week[0]}_{week[1]}\"\n",
    "    df = pd.read_csv(HEADLINES_PATH / f\"{week_str}.csv\")\n",
    "    manual_indices = [idx for idx, row in df.iterrows() if row[\"manual_headline\"] != \"NO_HEADLINE\"]\n",
    "\n",
    "    # load rankings for each LLM\n",
    "    rankings = {\n",
    "        llm: pd.read_csv(llm_to_manual_folder[llm] / f\"{week_str}.csv\")\n",
    "        for llm in llms\n",
    "    }\n",
    "\n",
    "    # compare ranks for each manual headline\n",
    "    for llm in llms:\n",
    "        abbrev = llm_to_abbrev[llm]\n",
    "        ranking = rankings[llm]\n",
    "\n",
    "        for mi in manual_indices:\n",
    "            manual_label = f\"M{mi}\"\n",
    "            llm_label = f\"{abbrev}{mi}\"\n",
    "\n",
    "            manual_rank = ranking.loc[ranking[\"label\"] == manual_label, \"rank\"].values[0]\n",
    "            llm_rank = ranking.loc[ranking[\"label\"] == llm_label, \"rank\"].values[0]\n",
    "            displacement = manual_rank - llm_rank\n",
    "\n",
    "            displacements[llm] += displacement\n",
    "            counts[llm] += 1\n",
    "\n",
    "# average displacement calculations\n",
    "avg_displacements = {\n",
    "    llm: displacements[llm] / counts[llm] for llm in llms\n",
    "}\n",
    "overall_avg = sum(displacements.values()) / sum(counts.values())\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Claude Sonnet 4': avg_displacements[\"claude\"],\n",
    "    'Gemini 2.5 Pro': avg_displacements[\"gemini\"],\n",
    "    'GPT-4.1 mini': avg_displacements[\"openai\"],\n",
    "    'LLM Average': overall_avg\n",
    "}\n",
    "\n",
    "# sort bars from highest to lowest\n",
    "sorted_items = sorted(data.items(), key=lambda x: x[1])\n",
    "labels, values = zip(*sorted_items)\n",
    "\n",
    "# bar colors\n",
    "color_map = {\n",
    "    'Claude Sonnet 4': \"#FF8585\",\n",
    "    'Gemini 2.5 Pro': \"#B0C4DE\",\n",
    "    'GPT-4.1 mini': \"#E2D49E\",\n",
    "    'LLM Average': \"#D8BFD8\"\n",
    "}\n",
    "colors = [color_map[label] for label in labels]\n",
    "\n",
    "# plot graph\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "bars = ax.barh(labels, values, color=colors)\n",
    "\n",
    "ax.axvline(0, color='black', linewidth=4)\n",
    "ax.set_xlim(-1, 8)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "# annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.annotate(\n",
    "        f'{width:.1f}',\n",
    "        xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "        xytext=(5 if width > 0 else -5, 0),\n",
    "        textcoords='offset points',\n",
    "        ha='left' if width > 0 else 'right',\n",
    "        va='center',\n",
    "        fontsize=20,\n",
    "        color='white' if width < 0 else 'black'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"headline_rank_difference.pdf\")\n",
    "# plt.savefig(RESULTS_PATH / \"headline_rank_difference.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca52c95",
   "metadata": {},
   "source": [
    "## Metric 2: Top-3/5 recall rate from LLM and expert selections\n",
    "Computes and displays the average recall rate of the true top-3 and true top-5 topics covered by the LLM and expert selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378eefe0",
   "metadata": {},
   "source": [
    "The folder `_interim/headlines/` should contain a `.csv` file of all headlines for each week. The true top-3 and true top-5 topics are obtained from the corresponding isolated LLM-generated headlines ranked by human evaluators. These are in the three folders:\n",
    "- `_interim/rankings/claude_only/`\n",
    "- `_interim/rankings/gemini_only/`\n",
    "- `_interim/rankings/openai_only/`\n",
    "\n",
    "The LLM's selection of topics is obtained from the corresponding folders:\n",
    "- `_interim/rankings/claude_ranks_claude/`\n",
    "- `_interim/rankings/claude_ranks_gemini/`\n",
    "- `_interim/rankings/claude_ranks_openai/`\n",
    "- `_interim/rankings/gemini_ranks_claude/`\n",
    "- `_interim/rankings/gemini_ranks_gemini/`\n",
    "- `_interim/rankings/gemini_ranks_openai/`\n",
    "- `_interim/rankings/openai_ranks_claude/`\n",
    "- `_interim/rankings/openai_ranks_gemini/`\n",
    "- `_interim/rankings/openai_ranks_openai/`\n",
    "\n",
    "The figures will be saved to the folder `results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINES_PATH = Path(\"../_interim/headlines/\")\n",
    "assert HEADLINES_PATH.exists()\n",
    "CLAUDE_ONLY_RANKING_PATH = Path(\"../_interim/rankings/claude_only/\")\n",
    "assert CLAUDE_ONLY_RANKING_PATH.exists()\n",
    "GEMINI_ONLY_RANKING_PATH = Path(\"../_interim/rankings/gemini_only/\")\n",
    "assert GEMINI_ONLY_RANKING_PATH.exists()\n",
    "OPENAI_ONLY_RANKING_PATH = Path(\"../_interim/rankings/openai_only/\")\n",
    "assert OPENAI_ONLY_RANKING_PATH.exists()\n",
    "\n",
    "CLAUDE_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/claude_ranks_claude/\")\n",
    "assert CLAUDE_RANKS_CLAUDE_PATH.exists()\n",
    "CLAUDE_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/claude_ranks_gemini/\")\n",
    "assert CLAUDE_RANKS_GEMINI_PATH.exists()\n",
    "CLAUDE_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/claude_ranks_openai/\")\n",
    "assert CLAUDE_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "GEMINI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/gemini_ranks_claude/\")\n",
    "assert GEMINI_RANKS_CLAUDE_PATH.exists()\n",
    "GEMINI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/gemini_ranks_gemini/\")\n",
    "assert GEMINI_RANKS_GEMINI_PATH.exists()\n",
    "GEMINI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/gemini_ranks_openai/\")\n",
    "assert GEMINI_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "OPENAI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/openai_ranks_claude/\")\n",
    "assert OPENAI_RANKS_CLAUDE_PATH.exists()\n",
    "OPENAI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/openai_ranks_gemini/\")\n",
    "assert OPENAI_RANKS_GEMINI_PATH.exists()\n",
    "OPENAI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/openai_ranks_openai/\")\n",
    "assert OPENAI_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "RESULTS_PATH = Path(\"../results/\")\n",
    "assert RESULTS_PATH.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f68631",
   "metadata": {},
   "source": [
    "Load in some variables and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"axes.unicode_minus\": False\n",
    "})\n",
    "\n",
    "\n",
    "llms = [\"claude\", \"gemini\", \"openai\"]\n",
    "llm_to_abbrev = {\"claude\": \"C\", \"gemini\": \"G\", \"openai\": \"O\"}\n",
    "llm_to_only_folder = {\"claude\": CLAUDE_ONLY_RANKING_PATH,\n",
    "                      \"gemini\": GEMINI_ONLY_RANKING_PATH,\n",
    "                      \"openai\": OPENAI_ONLY_RANKING_PATH}\n",
    "judge_rank_folder = {(\"claude\", \"claude\"): CLAUDE_RANKS_CLAUDE_PATH,\n",
    "                     (\"claude\", \"gemini\"): CLAUDE_RANKS_GEMINI_PATH,\n",
    "                     (\"claude\", \"openai\"): CLAUDE_RANKS_OPENAI_PATH,\n",
    "                     (\"gemini\", \"claude\"): GEMINI_RANKS_CLAUDE_PATH,\n",
    "                     (\"gemini\", \"gemini\"): GEMINI_RANKS_GEMINI_PATH,\n",
    "                     (\"gemini\", \"openai\"): GEMINI_RANKS_OPENAI_PATH,\n",
    "                     (\"openai\", \"claude\"): OPENAI_RANKS_CLAUDE_PATH,\n",
    "                     (\"openai\", \"gemini\"): OPENAI_RANKS_GEMINI_PATH,\n",
    "                     (\"openai\", \"openai\"): OPENAI_RANKS_OPENAI_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b83f08",
   "metadata": {},
   "source": [
    "Define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_ids(labels):\n",
    "    return set(int(label[1:]) for label in labels)\n",
    "\n",
    "def load_top_n_labels(week_str, llm, n):\n",
    "    df = pd.read_csv(llm_to_only_folder[llm] / f\"{week_str}.csv\")\n",
    "    return parse_label_ids(df.iloc[:n][\"label\"])\n",
    "\n",
    "def load_llm_ranking_indices(week_str, llm, judge, num_manual):\n",
    "    df = pd.read_csv(judge_rank_folder[(judge, llm)] / f\"{week_str}.csv\")\n",
    "    return parse_label_ids(df.iloc[:num_manual][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8319d7",
   "metadata": {},
   "source": [
    "Compute and display the recall rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall counters\n",
    "recall = {\n",
    "    \"manual_top3\": 0,\n",
    "    \"manual_top5\": 0,\n",
    "    \"claude_top3\": 0,\n",
    "    \"claude_top5\": 0,\n",
    "    \"gemini_top3\": 0,\n",
    "    \"gemini_top5\": 0,\n",
    "    \"openai_top3\": 0,\n",
    "    \"openai_top5\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# process each week\n",
    "for week_start, week_end in WEEKS:\n",
    "    week_str = f\"{week_start}_{week_end}\"\n",
    "\n",
    "\n",
    "    df = pd.read_csv(HEADLINES_PATH / f\"{week_str}.csv\")\n",
    "    manual_indices = set(df[df[\"manual_headline\"] != \"NO_HEADLINE\"].index)\n",
    "\n",
    "    # load top-3 and top-5 for each LLM\n",
    "    top3 = {llm: load_top_n_labels(week_str, llm, 3) for llm in llms}\n",
    "    top5 = {llm: load_top_n_labels(week_str, llm, 5) for llm in llms}\n",
    "\n",
    "    # count overlap with manual\n",
    "    recall[\"manual_top3\"] += sum(len(top3[llm] & manual_indices) for llm in llms)\n",
    "    recall[\"manual_top5\"] += sum(len(top5[llm] & manual_indices) for llm in llms)\n",
    "\n",
    "    # load LLM rankings judged by each LLM\n",
    "    num_manual = len(manual_indices)\n",
    "    for judge in llms:\n",
    "        judge_key_top3 = f\"{judge}_top3\"\n",
    "        judge_key_top5 = f\"{judge}_top5\"\n",
    "        for llm in llms:\n",
    "            pred_indices = load_llm_ranking_indices(week_str, llm, judge, num_manual)\n",
    "            recall[judge_key_top3] += len(top3[llm] & pred_indices)\n",
    "            recall[judge_key_top5] += len(top5[llm] & pred_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize recall values\n",
    "num_weeks = len(WEEKS)\n",
    "for key in recall:\n",
    "    if \"top3\" in key:\n",
    "        recall[key] /= (num_weeks * 3)\n",
    "    elif \"top5\" in key:\n",
    "        recall[key] /= (num_weeks * 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the graph\n",
    "group_labels = ['Gemini 2.5 Pro', 'Claude Sonnet 4', 'Expert', 'GPT-4.1']\n",
    "values_top3 = [\n",
    "    100 * recall[\"gemini_top3\"] / 3,\n",
    "    100 * recall[\"claude_top3\"] / 3,\n",
    "    100 * recall[\"manual_top3\"] / 3,\n",
    "    100 * recall[\"openai_top3\"] / 3\n",
    "]\n",
    "values_top5 = [\n",
    "    100 * recall[\"gemini_top5\"] / 3,\n",
    "    100 * recall[\"claude_top5\"] / 3,\n",
    "    100 * recall[\"manual_top5\"] / 3,\n",
    "    100 * recall[\"openai_top5\"] / 3\n",
    "]\n",
    "\n",
    "x = np.arange(len(group_labels))\n",
    "width = 0.35\n",
    "\n",
    "\n",
    "# colors\n",
    "color_top3 = {\n",
    "    'Claude Sonnet 4': \"#E6B3B3\",\n",
    "    'Gemini 2.5 Pro': \"#D4E6F1\",\n",
    "    'GPT-4.1': \"#FFF4C2\",\n",
    "    'Expert': \"#828282\"\n",
    "}\n",
    "color_top5 = {\n",
    "    'Claude Sonnet 4': \"#FF8585\",\n",
    "    'Gemini 2.5 Pro' : \"#B0C4DE\",\n",
    "    'GPT-4.1': \"#E2D49E\",\n",
    "    'Expert': \"#000000\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "bars_top3 = ax.bar(x - width/2, values_top3, width, color=[color_top3[g] for g in group_labels])\n",
    "bars_top5 = ax.bar(x + width/2, values_top5, width, color=[color_top5[g] for g in group_labels])\n",
    "\n",
    "ax.set_ylim(0, 70)\n",
    "ax.set_ylabel('Percentage', fontsize=25)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(group_labels, fontsize=25)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# bar annotations\n",
    "def annotate_bars(bars, label):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=20)\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height + 5, label,\n",
    "                ha='center', va='bottom', fontsize=20)\n",
    "\n",
    "annotate_bars(bars_top3, \"Top-3\")\n",
    "annotate_bars(bars_top5, \"Top-5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"recall_rate.pdf\")\n",
    "# plt.savefig(RESULTS_PATH / \"recall_rate.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489594c3",
   "metadata": {},
   "source": [
    "## Metric 3: Average rank of LLM and expert selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b88aab",
   "metadata": {},
   "source": [
    "The folder `_interim/headlines/` should contain a `.csv` file of all headlines for each week. The true rank of topics are obtained from the corresponding isolated LLM-generated headlines ranked by human evaluators. These are in the three folders:\n",
    "- `_interim/rankings/claude_only/`\n",
    "- `_interim/rankings/gemini_only/`\n",
    "- `_interim/rankings/openai_only/`\n",
    "\n",
    "The LLM's selection of topics is obtained from the corresponding folders:\n",
    "- `_interim/rankings/claude_ranks_claude/`\n",
    "- `_interim/rankings/claude_ranks_gemini/`\n",
    "- `_interim/rankings/claude_ranks_openai/`\n",
    "- `_interim/rankings/gemini_ranks_claude/`\n",
    "- `_interim/rankings/gemini_ranks_gemini/`\n",
    "- `_interim/rankings/gemini_ranks_openai/`\n",
    "- `_interim/rankings/openai_ranks_claude/`\n",
    "- `_interim/rankings/openai_ranks_gemini/`\n",
    "- `_interim/rankings/openai_ranks_openai/`\n",
    "\n",
    "The figures will be saved to the folder `results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINES_PATH = Path(\"../_interim/headlines/\")\n",
    "assert HEADLINES_PATH.exists()\n",
    "CLAUDE_ONLY_RANKING_PATH = Path(\"../_interim/rankings/claude_only/\")\n",
    "assert CLAUDE_ONLY_RANKING_PATH.exists()\n",
    "GEMINI_ONLY_RANKING_PATH = Path(\"../_interim/rankings/gemini_only/\")\n",
    "assert GEMINI_ONLY_RANKING_PATH.exists()\n",
    "OPENAI_ONLY_RANKING_PATH = Path(\"../_interim/rankings/openai_only/\")\n",
    "assert OPENAI_ONLY_RANKING_PATH.exists()\n",
    "\n",
    "CLAUDE_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/claude_ranks_claude/\")\n",
    "assert CLAUDE_RANKS_CLAUDE_PATH.exists()\n",
    "CLAUDE_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/claude_ranks_gemini/\")\n",
    "assert CLAUDE_RANKS_GEMINI_PATH.exists()\n",
    "CLAUDE_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/claude_ranks_openai/\")\n",
    "assert CLAUDE_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "GEMINI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/gemini_ranks_claude/\")\n",
    "assert GEMINI_RANKS_CLAUDE_PATH.exists()\n",
    "GEMINI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/gemini_ranks_gemini/\")\n",
    "assert GEMINI_RANKS_GEMINI_PATH.exists()\n",
    "GEMINI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/gemini_ranks_openai/\")\n",
    "assert GEMINI_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "OPENAI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/openai_ranks_claude/\")\n",
    "assert OPENAI_RANKS_CLAUDE_PATH.exists()\n",
    "OPENAI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/openai_ranks_gemini/\")\n",
    "assert OPENAI_RANKS_GEMINI_PATH.exists()\n",
    "OPENAI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/openai_ranks_openai/\")\n",
    "assert OPENAI_RANKS_OPENAI_PATH.exists()\n",
    "\n",
    "RESULTS_PATH = Path(\"../results/\")\n",
    "assert RESULTS_PATH.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e5c03",
   "metadata": {},
   "source": [
    "Load some variables and define some settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"axes.unicode_minus\": False\n",
    "})\n",
    "\n",
    "llms = [\"claude\", \"gemini\", \"openai\"]\n",
    "llm_to_abbrev = {\"claude\": \"C\",\n",
    "                 \"gemini\": \"G\",\n",
    "                 \"openai\": \"O\"}\n",
    "llm_to_only_folder = {\"claude\": CLAUDE_ONLY_RANKING_PATH,\n",
    "                      \"gemini\": GEMINI_ONLY_RANKING_PATH,\n",
    "                      \"openai\": OPENAI_ONLY_RANKING_PATH}\n",
    "judge_rank_folder = {(\"claude\", \"claude\"): CLAUDE_RANKS_CLAUDE_PATH,\n",
    "                     (\"claude\", \"gemini\"): CLAUDE_RANKS_GEMINI_PATH,\n",
    "                     (\"claude\", \"openai\"): CLAUDE_RANKS_OPENAI_PATH,\n",
    "                     (\"gemini\", \"claude\"): GEMINI_RANKS_CLAUDE_PATH,\n",
    "                     (\"gemini\", \"gemini\"): GEMINI_RANKS_GEMINI_PATH,\n",
    "                     (\"gemini\", \"openai\"): GEMINI_RANKS_OPENAI_PATH,\n",
    "                     (\"openai\", \"claude\"): OPENAI_RANKS_CLAUDE_PATH,\n",
    "                     (\"openai\", \"gemini\"): OPENAI_RANKS_GEMINI_PATH,\n",
    "                     (\"openai\", \"openai\"): OPENAI_RANKS_OPENAI_PATH}\n",
    "\n",
    "\n",
    "# initialization\n",
    "sum_manual_rank = sum_claude_rank = sum_gemini_rank = sum_openai_rank = 0\n",
    "manual_count = claude_count = gemini_count = openai_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aac478",
   "metadata": {},
   "source": [
    "Compute and display average ranks of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f367f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each week\n",
    "for week in WEEKS:\n",
    "    week_str = f\"{week[0]}_{week[1]}\"\n",
    "    df = pd.read_csv(HEADLINES_PATH / f\"{week_str}.csv\")\n",
    "    manual_indices = set(df[df[\"manual_headline\"] != \"NO_HEADLINE\"].index)\n",
    "\n",
    "    # load true rankings\n",
    "    true_df_dict = {\n",
    "        \"true_claude\": pd.read_csv(CLAUDE_ONLY_RANKING_PATH / f\"{week_str}.csv\"),\n",
    "        \"true_gemini\": pd.read_csv(GEMINI_ONLY_RANKING_PATH / f\"{week_str}.csv\"),\n",
    "        \"true_openai\": pd.read_csv(OPENAI_ONLY_RANKING_PATH / f\"{week_str}.csv\")\n",
    "    }\n",
    "\n",
    "    # load expert selections\n",
    "    for gen_llm in llms:\n",
    "        true = true_df_dict[f\"true_{gen_llm}\"]\n",
    "        manual_labels = [f\"{llm_to_abbrev[gen_llm]}{i}\" for i in manual_indices]\n",
    "\n",
    "        for _, row in true.iterrows():\n",
    "            if row[\"label\"] in manual_labels:\n",
    "                sum_manual_rank += row[\"rank\"]\n",
    "                manual_count += 1\n",
    "\n",
    "    # load LLM selections\n",
    "    for gen_llm in llms:\n",
    "        for ranking_llm in llms:\n",
    "            true = true_df_dict[f\"true_{gen_llm}\"]\n",
    "            ranking_path = Path(judge_rank_folder[(ranking_llm, gen_llm)] / f\"{week_str}.csv\")\n",
    "            ranking = pd.read_csv(ranking_path)\n",
    "\n",
    "            top_labels = ranking.loc[:len(manual_indices)-1, \"label\"]\n",
    "\n",
    "            for _, row in true.iterrows():\n",
    "                if row[\"label\"] in top_labels.values:\n",
    "                    if ranking_llm == \"claude\":\n",
    "                        sum_claude_rank += row[\"rank\"]\n",
    "                        claude_count += 1\n",
    "                    elif ranking_llm == \"gemini\":\n",
    "                        sum_gemini_rank += row[\"rank\"]\n",
    "                        gemini_count += 1\n",
    "                    elif ranking_llm == \"openai\":\n",
    "                        sum_openai_rank += row[\"rank\"]\n",
    "                        openai_count += 1\n",
    "\n",
    "# compute averages\n",
    "avg_manual_rank = sum_manual_rank / manual_count\n",
    "avg_claude_rank = sum_claude_rank / claude_count\n",
    "avg_gemini_rank = sum_gemini_rank / gemini_count\n",
    "avg_openai_rank = sum_openai_rank / openai_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the graph\n",
    "rank_data = {\n",
    "    'Claude Sonnet 4': avg_claude_rank,\n",
    "    'GPT-4.1': avg_openai_rank,\n",
    "    'Expert': avg_manual_rank,\n",
    "    'Gemini 2.5 Pro': avg_gemini_rank\n",
    "}\n",
    "\n",
    "# sort results by rank in ascending order\n",
    "sorted_items = sorted(rank_data.items(), key=lambda x: x[1])\n",
    "labels, ranks = zip(*sorted_items)\n",
    "\n",
    "# colors\n",
    "color_map = {\n",
    "    'Claude Sonnet 4': \"#FF8585\",\n",
    "    'Gemini 2.5 Pro' : \"#B0C4DE\",\n",
    "    'GPT-4.1': \"#E2D49E\",\n",
    "    'Expert': \"#000000\"\n",
    "}\n",
    "colors = [color_map[label] for label in labels]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "bars = ax.bar(labels, ranks, color=colors)\n",
    "\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_ylabel('Average Rank (Lower is Better)', fontsize=25)\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# annotate bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 5),\n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / \"average_rank.pdf\")\n",
    "# plt.savefig(RESULTS_PATH / \"average_rank.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e87f5",
   "metadata": {},
   "source": [
    "This concludes Module 5: LLM Auditing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
