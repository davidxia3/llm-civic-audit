{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507e4612",
   "metadata": {},
   "source": [
    "# Module 4: Human and LLM Headline Ranking\n",
    "- Step 1: Load headlines\n",
    "- Step 2: MANUALLY filter out some clearly trivial topics\n",
    "- Step 3: Create pairwise matchups\n",
    "- Step 4: Create headline label mappings\n",
    "- Step 5: Crowd-source human evaluation\n",
    "- Step 6: Obtain rankings from human evaluation results\n",
    "- Step 7: Generate LLM headline rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ea82e",
   "metadata": {},
   "source": [
    "Specify the week of meetings to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EDIT THIS\n",
    "MONDAY_DATE = \"YYYYMMDD\"\n",
    "FRIDAY_DATE = \"YYYYMMDD\"\n",
    "#### EDIT THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea97313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "WEEK = (MONDAY_DATE, FRIDAY_DATE)\n",
    "START_DATE = datetime.strptime(WEEK[0], \"%Y%m%d\")\n",
    "END_DATE = datetime.strptime(WEEK[1], \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4a7a1",
   "metadata": {},
   "source": [
    "## Step 1: Load headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf1d24",
   "metadata": {},
   "source": [
    "The LLM generated headlines should be saved in the folder `_interim/agenda_segments/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENDA_SEGMENTS_PATH = Path(\"../_interim/agenda_segments/\")\n",
    "assert AGENDA_SEGMENTS_PATH.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdb8bd",
   "metadata": {},
   "source": [
    "Load headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_headlines, gemini_headlines, openai_headlines, manual_headlines = [], [], [], []\n",
    "\n",
    "\n",
    "for aseg_file in AGENDA_SEGMENTS_PATH.glob(\"*.csv\"):\n",
    "        \n",
    "    # check if meeting took place in specified week\n",
    "    date_str = aseg_file.stem[:8]\n",
    "    try:\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # skip, meeting does not valid date\n",
    "        continue\n",
    "    if file_date < START_DATE or file_date > END_DATE:\n",
    "        # skip, meeting did not take place in specified week\n",
    "        continue\n",
    "\n",
    "    # load headlines\n",
    "    df = pd.read_csv(aseg_file)\n",
    "    for _, row in df.iterrows():\n",
    "        if row[f\"claude_headline\"] == \"NO_HEADLINE\" or row[f\"gemini_headline\"] == \"NO_HEADLINE\" or row[f\"openai_headline\"] == \"NO_HEADLINE\":\n",
    "            continue\n",
    "\n",
    "        manual_headlines.append(row[\"true_headline\"])\n",
    "        claude_headlines.append(row[\"claude_headline\"])\n",
    "        gemini_headlines.append(row[\"gemini_headline\"])\n",
    "        openai_headlines.append(row[\"openai_headline\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427dfa0",
   "metadata": {},
   "source": [
    "## Step 2: MANUALLY choose clearly unimportant topics to exclude (e.g. roll call, meeting adjournment, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_flags = []\n",
    "skipped_count = 0\n",
    "\n",
    "# just use Claude headlines as a proxy for topics\n",
    "# enter `s` to indicate to exclude the topic\n",
    "# enter anything else to include the topic\n",
    "print(len(claude_headlines))\n",
    "for headline in claude_headlines:\n",
    "    inp = input(headline + \": \")\n",
    "    if inp.strip().lower() == \"s\":\n",
    "        skip_flags.append(1)\n",
    "        skipped_count += 1\n",
    "    else:\n",
    "        skip_flags.append(0)\n",
    "\n",
    "\n",
    "# assemble and filter the df \n",
    "df = pd.DataFrame({\n",
    "    \"manual_headline\": manual_headlines,\n",
    "    \"claude_headline\": claude_headlines,\n",
    "    \"gemini_headline\": gemini_headlines,\n",
    "    \"openai_headline\": openai_headlines,\n",
    "    \"skip\": skip_flags\n",
    "})\n",
    "df = df[df[\"skip\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0dca7",
   "metadata": {},
   "source": [
    "The filtered headline dataframe will be saved to the folder `_interim/headlines/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINES_PATH = Path(\"../_interim/headlines/\")\n",
    "HEADLINES_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa3762",
   "metadata": {},
   "source": [
    "## Step 3: Create pairwise matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1132f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e4246",
   "metadata": {},
   "source": [
    "The dataframe with two columns, `Headline 1` and `Headline 2` containing pairwise matchups for human evaluators, are saved to the folder `_interim/matchups/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7785333",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCHUPS_PATH = Path(\"../_interim/matchups/\")\n",
    "MATCHUPS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd837bd5",
   "metadata": {},
   "source": [
    "Setup a helper function to randomize position of headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pairwise_question(matchups_df, h1, h2):\n",
    "    flip = random.randint(0, 1)\n",
    "    row = {\n",
    "        \"Headline 1\": h1 if flip == 0 else h2,\n",
    "        \"Headline 2\": h2 if flip == 0 else h1\n",
    "    }\n",
    "    \n",
    "    matchups_df.loc[len(matchups_df)] = row\n",
    "    return matchups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68e0c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total question pairs generated: 1761\n"
     ]
    }
   ],
   "source": [
    "LLM_CODE = {\"C\": \"claude\", \"G\": \"gemini\", \"O\": \"openai\"}\n",
    "\n",
    "matchups_df = pd.DataFrame(columns=[\"Headline 1\", \"Headline 2\"])\n",
    "df = pd.read_csv(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")\n",
    "\n",
    "# intra-model pairwise (Claude/Gemini/OpenAI)\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        for model in [\"C\", \"G\", \"O\"]:\n",
    "            matchups_df = add_pairwise_question(matchups_df, df.iloc[i][f\"{LLM_CODE[model]}_headline\"], df.iloc[j][f\"{LLM_CODE[model]}_headline\"])\n",
    "\n",
    "# manual vs llm pairs\n",
    "for i, row in df.iterrows():\n",
    "    manual = row[\"manual_headline\"]\n",
    "    if manual == \"NO_HEADLINE\":\n",
    "        continue\n",
    "    for j, other in df.iterrows():\n",
    "        if i == j:\n",
    "            continue\n",
    "        for model in [\"C\", \"G\", \"O\"]:\n",
    "            matchups_df = add_pairwise_question(matchups_df, manual, other[f\"{LLM_CODE[model]}_headline\"])\n",
    "\n",
    "# manual vs manual\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        m1, m2 = df.iloc[i][\"manual_headline\"], df.iloc[j][\"manual_headline\"]\n",
    "        if m1 == \"NO_HEADLINE\" or m2 == \"NO_HEADLINE\":\n",
    "            continue\n",
    "        matchups_df = add_pairwise_question(matchups_df, m1, m2)\n",
    "\n",
    "\n",
    "matchups_df.to_csv(MATCHUPS_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\", index=False)\n",
    "print(f\"total question pairs generated: {len(matchups_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115b60",
   "metadata": {},
   "source": [
    "## Step 4: Create label mapping to easily identify headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdc4fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc9a86",
   "metadata": {},
   "source": [
    "Label mappings are saved to the folder `_interim/headlines/` as `.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d51df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert HEADLINES_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_to_labels = {}\n",
    "labels_to_headlines = {}\n",
    "\n",
    "df = pd.read_csv(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")\n",
    "for idx, row in df.iterrows():\n",
    "    for prefix, headline in zip([\"M\", \"C\", \"G\", \"O\"], [\n",
    "        row[\"manual_headline\"], row[\"claude_headline\"],\n",
    "        row[\"gemini_headline\"], row[\"openai_headline\"]\n",
    "    ]):\n",
    "        if headline == \"NO_HEADLINE\" and prefix == \"M\":\n",
    "            continue\n",
    "        label = f\"{prefix}{idx}\"\n",
    "        headlines_to_labels[headline] = label\n",
    "        labels_to_headlines[label] = headline\n",
    "\n",
    "\n",
    "with open(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}_labels.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"headlines_to_labels\": headlines_to_labels,\n",
    "        \"labels_to_headlines\": labels_to_headlines\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be0cfe",
   "metadata": {},
   "source": [
    "## Step 5: Human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff46fb",
   "metadata": {},
   "source": [
    "The `.csv` file containing the pairwise matchups are now ready to export to Prolific. Once the evaluators have completed their selections, upload the `.csv` file containing their selections to the folder `___input/evaluations/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATIONS_PATH = Path(\"../___input/evaluations\")\n",
    "assert EVALUATIONS_PATH.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a5f66",
   "metadata": {},
   "source": [
    "## Step 6: Obtain rankings from pairwise matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cc16a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from trueskill import Rating, rate_1vs1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f30efb",
   "metadata": {},
   "source": [
    "Use the TrueSkill ranking algorithm to obtain rankings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfb45060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueskill_ranking(item_labels, comparisons, label_to_headline):\n",
    "    ratings = {item: Rating() for item in item_labels}\n",
    "\n",
    "    for winner, loser in comparisons:\n",
    "        if winner in ratings and loser in ratings:\n",
    "            ratings[winner], ratings[loser] = rate_1vs1(ratings[winner], ratings[loser])\n",
    "\n",
    "    ranked_items = sorted(ratings.items(), key=lambda x: x[1].mu, reverse=True)\n",
    "\n",
    "    z = 1.96  # 95% confidence\n",
    "    results = []\n",
    "\n",
    "    for rank, (label, rating) in enumerate(ranked_items, start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"label\": label,\n",
    "            \"headline\": label_to_headline.get(label, \"UNKNOWN\"),\n",
    "            \"score_mu\": round(rating.mu, 2),\n",
    "            \"score_sigma\": round(rating.sigma, 2),\n",
    "            \"score_95ci\": round(z * rating.sigma, 2),\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745dbe71",
   "metadata": {},
   "source": [
    "Rankings will be saved to the folder `_interim/rankings/` and to the appropriate subfolder: \n",
    "- `_interim/rankings/claude_only/`\n",
    "- `_interim/rankings/gemini_only/`\n",
    "- `_interim/rankings/openai_only/`\n",
    "- `_interim/rankings/claude_manual/`\n",
    "- `_interim/rankings/gemini_manual/`\n",
    "- `_interim/rankings/openai_manual/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUDE_ONLY_PATH = Path(\"../_interim/rankings/claude_only/\")\n",
    "CLAUDE_ONLY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "GEMINI_ONLY_PATH = Path(\"../_interim/rankings/gemini_only/\")\n",
    "GEMINI_ONLY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OPENAI_ONLY_PATH = Path(\"../_interim/rankings/openai_only/\")\n",
    "OPENAI_ONLY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLAUDE_MANUAL_PATH = Path(\"../_interim/rankings/claude_manual/\")\n",
    "CLAUDE_MANUAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "GEMINI_MANUAL_PATH = Path(\"../_interim/rankings/gemini_manual/\")\n",
    "GEMINI_MANUAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OPENAI_MANUAL_PATH = Path(\"../_interim/rankings/openai_manual/\")\n",
    "OPENAI_MANUAL_PATH.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a0532b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")\n",
    "manual_indices = [idx for idx, row in df.iterrows() if row[\"manual_headline\"] != \"NO_HEADLINE\"]\n",
    "total_headlines = len(df)\n",
    "\n",
    "# labels\n",
    "claude_labels = [f\"C{i}\" for i in range(total_headlines)]\n",
    "gemini_labels = [f\"G{i}\" for i in range(total_headlines)]\n",
    "openai_labels = [f\"O{i}\" for i in range(total_headlines)]\n",
    "\n",
    "manual_claude_labels = claude_labels + [f\"M{i}\" for i in manual_indices]\n",
    "manual_gemini_labels = gemini_labels + [f\"M{i}\" for i in manual_indices]\n",
    "manual_openai_labels = openai_labels + [f\"M{i}\" for i in manual_indices]\n",
    "\n",
    "with open(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}_labels.json\", \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "labels_to_headlines = label_map[\"labels_to_headlines\"]\n",
    "headlines_to_labels = label_map[\"headlines_to_labels\"]\n",
    "\n",
    "\n",
    "\n",
    "# human evaluation responses\n",
    "responses_df = pd.read_csv(EVALUATIONS_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")\n",
    "\n",
    "# obtain list of tuples representing the matchups, (winner id, loser id)\n",
    "comparisons = []\n",
    "for _, row in responses_df.iterrows():\n",
    "    hl1 = row[\"Headline 1\"]\n",
    "    hl2 = row[\"Headline 2\"]\n",
    "    winner = row[\"Annotator1_Response\"]\n",
    "\n",
    "    id1 = headlines_to_labels[hl1]\n",
    "    id2 = headlines_to_labels[hl2]\n",
    "\n",
    "    comparisons.append((id1, id2) if winner == \"Headline 1\" else (id2, id1))\n",
    "\n",
    "\n",
    "\n",
    "# different rankings to generate\n",
    "ranking_tasks = [\n",
    "    (\"claude_manual\", manual_claude_labels, CLAUDE_MANUAL_PATH),\n",
    "    (\"gemini_manual\", manual_gemini_labels, GEMINI_MANUAL_PATH),\n",
    "    (\"openai_manual\", manual_openai_labels, OPENAI_MANUAL_PATH),\n",
    "    (\"claude_only\", claude_labels, CLAUDE_ONLY_PATH),\n",
    "    (\"gemini_only\", gemini_labels, GEMINI_ONLY_PATH),\n",
    "    (\"openai_only\", openai_labels, OPENAI_ONLY_PATH),\n",
    "]\n",
    "\n",
    "\n",
    "for name, label_group, save_folder_path in ranking_tasks:\n",
    "    results = trueskill_ranking(label_group, comparisons, labels_to_headlines)\n",
    "    \n",
    "    save_path = save_folder_path / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\"\n",
    "\n",
    "    with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3a9b4",
   "metadata": {},
   "source": [
    "## Step 7: Generate LLM rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c379d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from trueskill import TrueSkill\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3179a",
   "metadata": {},
   "source": [
    "Load API keys and setup clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API keys\n",
    "claude_key = os.getenv(\"CLAUDE_KEY\")\n",
    "openai_key = os.getenv(\"GEMINI_KEY\")\n",
    "gemini_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "claude_client = anthropic.Anthropic(api_key=claude_key)\n",
    "openai_client = OpenAI(api_key=openai_key)\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbacbf0",
   "metadata": {},
   "source": [
    "Define the prompt to instruct the LLM to pick the more \"important\" headline. Definition of \"importance\" also given in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4885d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_comparison_prompt(headline1, headline2):\n",
    "    return f\"\"\"\n",
    "You will be shown two headlines from city council meetings.\n",
    "\n",
    "### Your Task\n",
    "Select the headline that is more important, using the definition below.\n",
    "\n",
    "### What Does “Important” Mean?\n",
    "A headline is important if:\n",
    "- It reflects a major change to the status quo,\n",
    "- OR it has a large impact on a large number of people,\n",
    "- OR it has a large impact on a marginalized group (e.g., people facing poverty, discrimination, or limited access to resources),\n",
    "- OR it covers an issue that is especially newsworthy due to its civic relevance, urgency, or long-term consequences.\n",
    "\n",
    "### Consider These Factors\n",
    "- **Scope**: How many people in the city are affected?\n",
    "- **Depth**: How significant or lasting is the impact?\n",
    "- **Equity**: Does it affect vulnerable or underserved communities?\n",
    "\n",
    "---\n",
    "\n",
    "### Compare the Headlines Below\n",
    "\n",
    "Headline 1: {headline1}\n",
    "Headline 2: {headline2}\n",
    "\n",
    "---\n",
    "\n",
    "Your output should be a single line: either `Headline 1` or `Headline 2` — no explanation.\n",
    "\n",
    "---\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Example 1**\n",
    "Headline 1: City Council Approves $20 Million Affordable Housing Project  \n",
    "Headline 2: Council Discusses Adding Public Art  \n",
    "**More Important**: Headline 1\n",
    "\n",
    "**Example 2**\n",
    "Headline 1: City Declares 'Local History Month'  \n",
    "Headline 2: Council Votes to Close Health Clinic Despite Protests  \n",
    "**More Important**: Headline 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b3f71",
   "metadata": {},
   "source": [
    "Define some helper functions to feed prompts to LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_headlines_claude(h1, h2):\n",
    "    prompt = make_comparison_prompt(h1, h2)\n",
    "    response = claude_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=64,\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text.strip()\n",
    "\n",
    "def compare_headlines_gemini(h1, h2):\n",
    "    prompt = make_comparison_prompt(h1, h2)\n",
    "    model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "    return model.generate_content(prompt).text.strip()\n",
    "\n",
    "def compare_headlines_openai(h1, h2):\n",
    "    prompt = make_comparison_prompt(h1, h2)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=64\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaf189",
   "metadata": {},
   "source": [
    "Each of the three LLMs will produce a ranking for each of the three lists of LLM generated headlines. Thus, there will be a total of nine ranking lists. The rankings will be saved in `_interim/rankings/` and under the appropriate subfolder: \n",
    "- `_interim/rankings/claude_ranks_claude/`\n",
    "- `_interim/rankings/claude_ranks_gemini/`\n",
    "- `_interim/rankings/claude_ranks_openai/`\n",
    "- `_interim/rankings/gemini_ranks_claude/`\n",
    "- `_interim/rankings/gemini_ranks_gemini/`\n",
    "- `_interim/rankings/gemini_ranks_openai/`\n",
    "- `_interim/rankings/openai_ranks_claude/`\n",
    "- `_interim/rankings/openai_ranks_gemini/`\n",
    "- `_interim/rankings/openai_ranks_openai/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986794e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUDE_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/claude_ranks_claude/\")\n",
    "CLAUDE_RANKS_CLAUDE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "CLAUDE_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/claude_ranks_gemini/\")\n",
    "CLAUDE_RANKS_GEMINI_PATH.mkdir(parents=True, exist_ok=True)\n",
    "CLAUDE_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/claude_ranks_openai/\")\n",
    "CLAUDE_RANKS_OPENAI_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GEMINI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/gemini_ranks_claude/\")\n",
    "GEMINI_RANKS_CLAUDE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "GEMINI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/gemini_ranks_gemini/\")\n",
    "GEMINI_RANKS_GEMINI_PATH.mkdir(parents=True, exist_ok=True)\n",
    "GEMINI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/gemini_ranks_openai/\")\n",
    "GEMINI_RANKS_OPENAI_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPENAI_RANKS_CLAUDE_PATH = Path(\"../_interim/rankings/openai_ranks_claude/\")\n",
    "OPENAI_RANKS_CLAUDE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OPENAI_RANKS_GEMINI_PATH = Path(\"../_interim/rankings/openai_ranks_gemini/\")\n",
    "OPENAI_RANKS_GEMINI_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OPENAI_RANKS_OPENAI_PATH = Path(\"../_interim/rankings/openai_ranks_openai/\")\n",
    "OPENAI_RANKS_OPENAI_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load headlines\n",
    "df = pd.read_csv(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\")\n",
    "claude_headlines = df[\"claude_headline\"].tolist()\n",
    "gemini_headlines = df[\"gemini_headline\"].tolist()\n",
    "openai_headlines = df[\"openai_headline\"].tolist()\n",
    "\n",
    "headlines_by_model = {\n",
    "    \"claude\": claude_headlines,\n",
    "    \"gemini\": gemini_headlines,\n",
    "    \"openai\": openai_headlines\n",
    "}\n",
    "\n",
    "# load headline labels\n",
    "with open(HEADLINES_PATH / f\"{MONDAY_DATE}_{FRIDAY_DATE}_labels.json\", \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "labels_to_headlines = label_map[\"labels_to_headlines\"]\n",
    "headlines_to_labels = label_map[\"headlines_to_labels\"]\n",
    "\n",
    "\n",
    "# the generating LLM is the LLM that generated the headlines\n",
    "# the ranking LLM is the LLM that will be doing the pairwise comparisons\n",
    "ranking_tasks = [\n",
    "    (\"claude\", \"claude\", CLAUDE_RANKS_CLAUDE_PATH),\n",
    "    (\"claude\", \"gemini\", CLAUDE_RANKS_GEMINI_PATH),\n",
    "    (\"claude\", \"openai\", CLAUDE_RANKS_OPENAI_PATH),\n",
    "\n",
    "    (\"gemini\", \"claude\", GEMINI_RANKS_CLAUDE_PATH),\n",
    "    (\"gemini\", \"gemini\", GEMINI_RANKS_GEMINI_PATH),\n",
    "    (\"gemini\", \"openai\", GEMINI_RANKS_OPENAI_PATH),\n",
    "\n",
    "    (\"openai\", \"claude\", OPENAI_RANKS_CLAUDE_PATH),\n",
    "    (\"openai\", \"gemini\", OPENAI_RANKS_GEMINI_PATH),\n",
    "    (\"openai\", \"openai\", OPENAI_RANKS_OPENAI_PATH),\n",
    "]\n",
    "\n",
    "for gen_llm, rank_llm, save_folder_path in ranking_tasks:\n",
    "    headlines = headlines_by_model[gen_llm]\n",
    "    labels = [headlines_to_labels[hl] for hl in headlines]\n",
    "\n",
    "    # define trueskill environment\n",
    "    ts = TrueSkill(draw_probability=0)\n",
    "    ratings = {h: ts.create_rating() for h in headlines}\n",
    "\n",
    "\n",
    "    # run pairwise comparisons\n",
    "    pairs = list(combinations(headlines, 2))\n",
    "\n",
    "    for i, (h1, h2) in enumerate(pairs, 1):\n",
    "        time.sleep(5)  # rate limiting\n",
    "\n",
    "        # choose LLM for ranking\n",
    "        if rank_llm == \"claude\":\n",
    "            winner = compare_headlines_claude(h1, h2)\n",
    "        elif rank_llm == \"gemini\":\n",
    "            winner = compare_headlines_gemini(h1, h2)\n",
    "        elif rank_llm == \"openai\":\n",
    "            winner = compare_headlines_openai(h1, h2)\n",
    "\n",
    "        # update ratings based on selection\n",
    "        if winner == \"Headline 1\":\n",
    "            ratings[h1], ratings[h2] = ts.rate_1vs1(ratings[h1], ratings[h2])\n",
    "        elif winner == \"Headline 2\":\n",
    "            ratings[h2], ratings[h1] = ts.rate_1vs1(ratings[h2], ratings[h1])\n",
    "        else:\n",
    "            raise ValueError(f\"!!!! INVALID LLM SELECTION: {winner}\")\n",
    "\n",
    "        print(f\"[{i}/{len(pairs)}] {headlines_to_labels[h1]} vs {headlines_to_labels[h2]} → {winner}\")\n",
    "\n",
    "\n",
    "\n",
    "    # create final ranking and save\n",
    "    ranked = sorted(ratings.items(), key=lambda x: x[1].mu, reverse=True)\n",
    "    z = 1.96  # 95% confidence interval\n",
    "\n",
    "    results = []\n",
    "    print(\"\\nfinal rankings:\")\n",
    "    for i, (headline, rating) in enumerate(ranked, 1):\n",
    "        label = headlines_to_labels[headline]\n",
    "        ci = z * rating.sigma\n",
    "        print(f\"{i}. {label} — {headline} (score: {rating.mu:.2f} ± {ci:.2f})\")\n",
    "        results.append({\n",
    "            \"rank\": i,\n",
    "            \"label\": label,\n",
    "            \"headline\": headline,\n",
    "            \"score_mu\": round(rating.mu, 2),\n",
    "            \"score_sigma\": round(rating.sigma, 2),\n",
    "            \"score_95ci\": round(ci, 2)\n",
    "        })\n",
    "\n",
    "    # save to csv\n",
    "    with open(save_folder_path / f\"{MONDAY_DATE}_{FRIDAY_DATE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046383d",
   "metadata": {},
   "source": [
    "This concludes Module 4: Headline Ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
